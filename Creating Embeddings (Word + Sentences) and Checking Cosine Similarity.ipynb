{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e68855b-feb1-4169-8693-ec847339fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim   \n",
    "                      # gensim is capable of implementing algorithms for discovering topics in a collection of documents like \n",
    "                      # 1. Latent Semantic Analysis (LSA), \n",
    "                      # 2. Latent Dirichlet Allocation (LDA)\n",
    "                      # 3. Hierarchical Dirichlet Process (HDP) \n",
    "                      # Offers tools for training and loading word vectors, including:\n",
    "                            # 1. Word2Vec\n",
    "                            # 2. FastText\n",
    "                            # 3. Pretrained models like Google News Word2Vec\n",
    "                      # Find the most similar words or documents using cosine similarity in vector space. Example Use Cases:\n",
    "                      # 1. Document Clustering (e.g., grouping news articles by topic), \n",
    "                      # 2. Semantic Search (e.g., retrieving similar questions from a Q&A database), \n",
    "                      # 3. Recommendation Systems (e.g., finding similar products based on descriptions)\n",
    "                      # 4. Word Similarity Tasks (e.g., “king” is to “queen” as “man” is to “woman”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdf268f-51b7-4714-9d87-b9e0083d0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c29ac1-7323-49b4-a94e-414d7214613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader # allows to easily download and load pre-trained models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2560932e-dbbf-4b42-8579-30b776b15e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['corpora', 'models'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.downloader.info().keys() # returns keys of the metadata dictionary describing the models and datasets available \n",
    "                                 # for download via Gensim's downloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb007f0c-28ad-4d4f-9efe-1b99fcd5dbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
       "  'file_size': 1005007116,\n",
       "  'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
       "  'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
       "   'https://arxiv.org/abs/1712.09405',\n",
       "   'https://arxiv.org/abs/1607.01759'],\n",
       "  'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
       "  'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
       "  'parts': 1},\n",
       " 'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
       "  'file_size': 1225497562,\n",
       "  'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
       "  'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
       "  'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
       "   'https://github.com/commonsense/conceptnet-numberbatch',\n",
       "   'http://conceptnet.io/'],\n",
       "  'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
       "  'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-ruscorpora-300': {'num_records': 184973,\n",
       "  'file_size': 208427381,\n",
       "  'base_dataset': 'Russian National Corpus (about 250M words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
       "  'parameters': {'dimension': 300, 'window_size': 10},\n",
       "  'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
       "  'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
       "  'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
       "   'http://rusvectores.org/en/',\n",
       "   'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
       "  'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
       "  'file_name': 'word2vec-ruscorpora-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-google-news-300': {'num_records': 3000000,\n",
       "  'file_size': 1743563840,\n",
       "  'base_dataset': 'Google News (about 100 billion words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
       "  'license': 'not found',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       "  'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "   'https://arxiv.org/abs/1301.3781',\n",
       "   'https://arxiv.org/abs/1310.4546',\n",
       "   'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       "  'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       "  'file_name': 'word2vec-google-news-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-50': {'num_records': 400000,\n",
       "  'file_size': 69182535,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       "  'file_name': 'glove-wiki-gigaword-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-100': {'num_records': 400000,\n",
       "  'file_size': 134300434,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
       "  'file_name': 'glove-wiki-gigaword-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-200': {'num_records': 400000,\n",
       "  'file_size': 264336934,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
       "  'file_name': 'glove-wiki-gigaword-200.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-300': {'num_records': 400000,\n",
       "  'file_size': 394362229,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
       "  'file_name': 'glove-wiki-gigaword-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-25': {'num_records': 1193514,\n",
       "  'file_size': 109885004,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 25},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
       "  'file_name': 'glove-twitter-25.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-50': {'num_records': 1193514,\n",
       "  'file_size': 209216938,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
       "  'file_name': 'glove-twitter-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-100': {'num_records': 1193514,\n",
       "  'file_size': 405932991,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
       "  'file_name': 'glove-twitter-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-200': {'num_records': 1193514,\n",
       "  'file_size': 795373100,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
       "  'file_name': 'glove-twitter-200.gz',\n",
       "  'parts': 1},\n",
       " '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
       "  'parameters': {'dimensions': 50},\n",
       "  'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
       "  'read_more': [],\n",
       "  'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
       "  'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
       "  'parts': 1}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.downloader.info()['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482642bd-b05c-46af-b6a7-ec7d23ea66ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.downloader.info()['models'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ba62b5-a4fb-42ac-997a-86d22a1af62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 3000000,\n",
       " 'file_size': 1743563840,\n",
       " 'base_dataset': 'Google News (about 100 billion words)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'parameters': {'dimension': 300},\n",
       " 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       " 'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "  'https://arxiv.org/abs/1301.3781',\n",
       "  'https://arxiv.org/abs/1310.4546',\n",
       "  'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       " 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       " 'file_name': 'word2vec-google-news-300.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.downloader.info()['models']['word2vec-google-news-300']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5632a5-eaa6-46d7-915f-3e57ad055f5d",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e399584d-1014-425e-a891-75d0cfea0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_2_Vec = gensim.downloader.load('word2vec-google-news-300') # Note: 300 represnts 300-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37232a22-2e87-49d1-8f5c-b50fd1f1543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_representation = Word_2_Vec.get_vector('Continent')  # fetches the vector representation (a 300-dimensional NumPy array) of \n",
    "                                                            # the word 'continent' from the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd6cafd-5294-4f47-a801-730f21bf69f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('continent', 0.7036794424057007),\n",
       " ('Europe', 0.5909315347671509),\n",
       " ('Africa', 0.5554481744766235),\n",
       " ('British_Isles', 0.5482714176177979),\n",
       " ('continental', 0.5253060460090637),\n",
       " ('Iberian_Peninsula', 0.5039645433425903),\n",
       " ('European', 0.48930081725120544),\n",
       " ('Scandinavia', 0.4872168004512787),\n",
       " ('Iberian_peninsula', 0.4815359115600586),\n",
       " ('continental_Europe', 0.4758695960044861)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word_2_Vec.most_similar('Continent') # returns the top words most similar to 'emperor' based on cosine similarity of word embeddings \n",
    "                                     # in the word2vec_300 model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef16c4-1973-4d92-9e2f-837cf2798471",
   "metadata": {},
   "source": [
    "### Calculating Cosine Similarity Between 2 Vectors (Words---> Word Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8773247a-1d8f-4874-af88-7fa1bbeb4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "emperor = Word_2_Vec.get_vector('King')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832d8f4b-90e5-4aa4-8afa-c879f6b0e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "empress = Word_2_Vec.get_vector('Queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68caa6b1-e953-48d7-8173-2f9b0e16c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba77982-0c04-4596-be10-0e6d576a6df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5157251]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(emperor.reshape(1,300),empress.reshape(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2765f4d8-5de9-44ea-b5c2-ec1d5efc068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knight = Word_2_Vec.get_vector('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "657ed0a5-97f2-46ce-afcf-85c1de959ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lady = Word_2_Vec.get_vector('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89bdc977-bc08-436b-9620-9a0cd14b9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "lady = knight-man+woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af5f2c89-3b30-4126-8cbe-88a3ef4f5070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76640135]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(knight.reshape(1,300), lady.reshape(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b246ef60-700c-4d59-b132-16080c75faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 2nd example\n",
    "Rhine= Word_2_Vec.get_vector('Rhine') # Rhine is a major river in Germany\n",
    "Siene = Word_2_Vec.get_vector('Siene') # Siene is a major river in France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8afde084-99e4-45ef-866d-09c8e957a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Germany  = Word_2_Vec.get_vector('Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5304593-2d50-40a5-a952-08fc414c12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "France = Word_2_Vec.get_vector('France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7b6dbcc-9f53-498f-881b-3fbe7fe222cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6270757]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = Germany-Rhine + Siene\n",
    "cosine_similarity(Germany.reshape(1,300), France.reshape(1,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ec9c0-7168-450c-8440-cb8fc360d1f4",
   "metadata": {},
   "source": [
    "### Calculating Cosine Similarity between 2 Sentences (Sentence Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f37667-739d-4c9b-b3fd-939690fb6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"India’s surgical strikes were a calculated and restrained response, rather than an act of full-scale warfare\"\n",
    "second_sentence = \"India’s surgical strikes were a precise and restrained action, aimed at addressing specific threats without escalating into full-scale warfare\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f47cd79b-0207-4ba5-af71-4642acbcf661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India’s',\n",
       " 'surgical',\n",
       " 'strikes',\n",
       " 'were',\n",
       " 'a',\n",
       " 'calculated',\n",
       " 'and',\n",
       " 'restrained',\n",
       " 'response,',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'an',\n",
       " 'act',\n",
       " 'of',\n",
       " 'full-scale',\n",
       " 'warfare']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "393ab120-a1a5-4515-a8d1-ddc210de4f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India’s',\n",
       " 'surgical',\n",
       " 'strikes',\n",
       " 'were',\n",
       " 'a',\n",
       " 'precise',\n",
       " 'and',\n",
       " 'restrained',\n",
       " 'action,',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'addressing',\n",
       " 'specific',\n",
       " 'threats',\n",
       " 'without',\n",
       " 'escalating',\n",
       " 'into',\n",
       " 'full-scale',\n",
       " 'warfare']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e27137de-3b45-42cd-aff9-4fe343ae8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_embedding = Word_2_Vec.get_mean_vector(first_sentence.split())\n",
    "second_sentence_embedding = Word_2_Vec.get_mean_vector(second_sentence.split())\n",
    "# .get_mean_vector looks up the Word2Vec embedding for each word following split. It then, \n",
    "# computes the mean (average) vector of all word embeddings in the sentence.\n",
    "# Why use .get_mean_vector? Because, Word2Vec gives embeddings for individual words, but many tasks \n",
    "# (like sentence similarity or classification) require a single vector per sentence.\n",
    "# So we take the average of all the word vectors in the sentence — a common technique called:\n",
    "# Mean pooling or sentence embedding via averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fce9001-4d29-4895-a468-2c122be3e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7308821]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(first_sentence_embedding.reshape(1,300), second_sentence_embedding.reshape(1,300))\n",
    "# Reshaping since cosine-similarity requires 2-dimensional input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d96b3-339f-43c2-abd4-d1c155a7237d",
   "metadata": {},
   "source": [
    "### Training your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b49bf984-19c7-4ce2-926b-9e010e39ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown # The Brown Corpus is a standard, well-known collection of English text, created in the 1960s. \n",
    "                              # It was the first million-word electronic corpus of American English, and \n",
    "                              # it's widely used in linguistics and NLP research. It contains texts from 500 different sources, \n",
    "                              # grouped by genre, including:\n",
    "                                                            # 1. News\n",
    "                                                            # 2. Editorials\n",
    "                                                            # 3. Fiction\n",
    "                                                            # 4. Romance\n",
    "                                                            # 5. Religion\n",
    "                                                            # 6. Government\n",
    "                                                            # 7. Science\n",
    "# All categories (genres)\n",
    "#    brown.categories()\n",
    "# → ['adventure', 'editorial', 'fiction', 'government', 'hobbies', ...]\n",
    "\n",
    "# Words from a specific category\n",
    "# brown.words(categories='news')\n",
    "\n",
    "# Sentences from the corpus\n",
    "# brown.sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e78bba2-0de4-4446-9168-eb1a9c0e6131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However',\n",
       " ',',\n",
       " 'the',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'it',\n",
       " 'believes',\n",
       " '``',\n",
       " 'these',\n",
       " 'two',\n",
       " 'offices',\n",
       " 'should',\n",
       " 'be',\n",
       " 'combined',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " 'and',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'administration',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(brown.sents())[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16421ed1-efe3-4cff-8cb2-5ebf6e578204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' .\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(brown.sents()[8])   # Retrieves the 8th sentence (Python uses 0-based indexing) from the Brown corpus.\n",
    "                             # Joins the list of words in that sentence into a single string with spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "038c4085-34c9-46a4-82c9-cb181b73b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating your own model\n",
    "my_model = gensim.models.Word2Vec(brown.sents(), vector_size = 25, epochs = 10, window = 10)\n",
    "\n",
    "# What we are doing above is creating a Word2Vec model using the Brown corpus with specific hyperparameters. \n",
    "# Here's a detailed breakdown of this line:\n",
    "                                            # brown.sents()\t---> The training data: tokenized sentences from the Brown corpus\n",
    "                                            # vector_size=20 ---> Each word will be represented by a 20-dimensional vector\n",
    "                                            # epochs=10\t---> The model will pass through the entire corpus 10 times (training cycles)\n",
    "                                            # window=10\tThe context window: how many words before and after a target word to consider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9440a4b-ae78-4fb1-8ad0-a174a846eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4107752 ,  2.0186083 , -1.7169677 ,  1.0043945 ,  0.5253399 ,\n",
       "       -0.69897914, -1.1418812 ,  0.24403727, -0.6537245 , -0.7337003 ,\n",
       "        0.33038673,  1.6087105 , -0.926237  , -0.70416117,  0.96861845,\n",
       "        1.1036601 ,  0.17073087,  0.18519805,  0.21197456,  0.6128516 ,\n",
       "        0.1080151 , -1.665362  , -0.27400574,  2.0222769 ,  0.51745445],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.wv.get_vector('faith')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4f23f5f-0ee0-4895-a6aa-a5816c4f47fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('defining', 0.8846367597579956),\n",
       " ('destiny', 0.8842817544937134),\n",
       " ('diplomacy', 0.8783668875694275),\n",
       " ('excellence', 0.8760520815849304),\n",
       " ('congregation', 0.8760414123535156),\n",
       " ('enterprise', 0.8731026649475098),\n",
       " ('participation', 0.8721271753311157),\n",
       " ('aesthetic', 0.8699518442153931),\n",
       " ('biology', 0.8696659803390503),\n",
       " ('aims', 0.8681924343109131)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.wv.most_similar('worship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80c265-5dc5-4053-bdc5-89ebc4a78b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
